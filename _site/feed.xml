<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.0.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-10-19T23:05:48+02:00</updated><id>/</id><title>Yifan Jiang</title><subtitle>My personal homepage.</subtitle><author><name>Yifan Jiang</name></author><entry><title>Single Image Depth Estimation using PyTorch</title><link href="/post/single-image-depth/" rel="alternate" type="text/html" title="Single Image Depth Estimation using PyTorch" /><published>2018-06-19T00:00:00+02:00</published><updated>2018-06-19T00:00:00+02:00</updated><id>/post/single-image-depth</id><content type="html" xml:base="/post/single-image-depth/">&lt;p&gt;There are a few ways to measure distances to objects: LIDAR, stereo vision,
ultrasonic sensors, and so on. What about just using a single RGB image and
estimate the depth of each pixel, using a convolution neural network (CNN)?
There has been many publications on this topic, such as [1,2,3]. Here, together
with a friend, I implemented the algorithm of [2], trained it from scratch and
tested it, using PyTorch.&lt;/p&gt;

&lt;p&gt;The code is in my GitHub repository: &lt;a href=&quot;https://github.com/YifanJiangPolyU/single_image_depth_pytorch&quot;&gt;https://github.com/YifanJiangPolyU/single_image_depth_pytorch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although [2] estimates both per-pixel depth and surface normals, we only
experimented depth estimation here.&lt;/p&gt;

&lt;p&gt;This work is done in collaboration with &lt;a href=&quot;https://github.com/tongguanc&quot;&gt;Guanchun Tong&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-neural-network&quot;&gt;The Neural Network&lt;/h2&gt;

&lt;p&gt;This is a multi-scale CNN model that estimates the depth of each pixel in a RGB
input image.&lt;/p&gt;

&lt;p&gt;The model consists of 3 different scales. Each scale is a set of convolution
and pooling layers that processes a scaled-down copy of the original input
image. The input image (304x228) is first processed by the scale 1 and 2,
creating a small-size estimation (74x55). Then, the small-sized estimation is
concatenated with the scale-down version of the input image, and is further
processed by scale 3, creating an estimation of a larger size (147x109). The
structure is exactly the same as [2], and is illustrated in figure below:&lt;/p&gt;

&lt;figure style=&quot;width: 800px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2018-06-19-single-image-depth/model.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1. Model Structure &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The NYUDepth v2 dataset [4] is used to train this model. Unfortunately, due to
disk space limitations, the entire dataset could not be used for training.
Instead, a densely labeled subset of NYUDepth v2 is used (containing 1499
images). The dataset is available &lt;a href=&quot;https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;95% of the images in the densely labeled subset of NYUDepth v2 are used for
training, and the remaining 5% are used for validation.&lt;/p&gt;

&lt;h3 id=&quot;loss-functions&quot;&gt;Loss Functions&lt;/h3&gt;

&lt;p&gt;The loss functions used for training are taken from [1].&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;The training takes place in two stages. First, scale 1 and 2 are trained as
shown in the following flow chart:&lt;/p&gt;

&lt;figure style=&quot;width: 200px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2018-06-19-single-image-depth/flow-chart-training.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2. Training Flowchart, Scale 1&amp;amp;2 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Then, scale 3 is trained. The depth loss described earlier is simply
back-propagated. Scale 3 is trained for at most 100 epochs, and the training is
terminated early if performance stops improving.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The resulting accuracy is worse compared to [2], which is to be expected since
the training dataset is much smaller. Baselines of comparisons are detailed in
[1]. The results are shown below:&lt;/p&gt;

&lt;figure style=&quot;width: 600px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2018-06-19-single-image-depth/table-res.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3. Depth Prediction Results &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Results for few sample images are compared below. Our model captures large-scale
geometries correctly, but almost all details are lost. In comparison, [2]
achieves good estimation even for fine details.&lt;/p&gt;

&lt;figure style=&quot;width: 800px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2018-06-19-single-image-depth/compare-depth.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4. From left to right: Input RGB image, ours, Eigen[2], ground truth &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Histograms below show the distribution of per-pixel depth estimation errors.&lt;/p&gt;

&lt;figure style=&quot;width: 800px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2018-06-19-single-image-depth/compare-hist.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5. Distribution of per-pixel estimation errors. From left to right: Ours, Eigen[2] &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;E. David. et al. Depth map prediction from a single image using a multi-scale
deep network. NIPS, 2014.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E. David and F. Rob. Predicting depth,surfacen ormals and semantic labels
with a common multi-scale convolutional architecture. ICCV, 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;C. Godard, O. M. Aodha, and G. J. Brostow. Unsupervised monocular depth
estimation with left-right consistency. CVPR, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;P. Kohli, N. Silberman, D. Hoiem, and R. Fergus. Indoor segmentation and
support inference from rgbd images. ECCV, 2012.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><category term="tech" /><summary>There are a few ways to measure distances to objects: LIDAR, stereo vision,
ultrasonic sensors, and so on. What about just using a single RGB image and
estimate the depth of each pixel, using a convolution neural network (CNN)?
There has been many publications on this topic, such as [1,2,3]. Here, together
with a friend, I implemented the algorithm of [2], trained it from scratch and
tested it, using PyTorch.</summary></entry><entry><title>Object Tracking based on Color with OpenCV</title><link href="/post/OpenVC-Color-Tracking/" rel="alternate" type="text/html" title="Object Tracking based on Color with OpenCV" /><published>2018-02-10T00:00:00+01:00</published><updated>2018-02-10T00:00:00+01:00</updated><id>/post/OpenVC-Color-Tracking</id><content type="html" xml:base="/post/OpenVC-Color-Tracking/">&lt;p&gt;OpenCV is a really powerful tool for computer vision developers. Combined with
python, prototyping and deployment become really fast. I recently revisited some
basic features of OpenCV, and re-discovered this little project I did a few
years ago.&lt;/p&gt;

&lt;p&gt;This application detects from a stream of video areas that have a predefined color,
in this case, green. Green areas are identified and labeled automatically, and
the centroid of the area is computed and labeled with a red dot. The results are
shown in video below:&lt;/p&gt;

&lt;iframe width=&quot;1294&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/VmGkj5CtbUY&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The realization of this function involves a number of steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A video stream is captured from a webcam, and is feed to the algorithm a frame at a time. The program processes the video stream in real time.&lt;/li&gt;
  &lt;li&gt;The frame is smoothed using a Gaussian kernel, and is then transformed from RGB color space to HSV color space.&lt;/li&gt;
  &lt;li&gt;The frame is thresholded by the Hue value at each pixel, creating a binary image.&lt;/li&gt;
  &lt;li&gt;Contours are detected in the thresholded binary image, and the centroid is computed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The HSV (Hue-Saturation-Value) color space is preferred over RGB, because the
Hue axis of HSV is directly related to the perceived color of a pixel. In
contrast, the relationship between the perceived color and the RGB values is
more complicated. In this program, “green” is defined as a Hue value between 50
and 80 (255 is maximum).&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;/images/2018-02-10-OpenVC-Color-Tracking/hsv.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.1 HSV Color Space&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Due to variations in lighting conditions and reflections, using a single set of
hue thresholds results in very bad tracking robustness. Therefore, an
adaptive thresholding method is implemented and is found to greatly improve
tracking robustness. The adaptive thresholding is executed as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Thresholds are initialized at &lt;code class=&quot;highlighter-rouge&quot;&gt;hue_min=50&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;hue_max=80&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;When a new frame is processed, identify the centroid of the largest green area, and store the Hue value at the centroid (name it &lt;code class=&quot;highlighter-rouge&quot;&gt;hue_central&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Update the thresholds as follows: &lt;code class=&quot;highlighter-rouge&quot;&gt;hue_min=hue_central-10&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;hue_max=hue_central+10&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The adaptive thresholding method ensures that the algorithm consistently tracks
the largest green area in the video stream. The method fails horribly, however,
when lighting conditions change too drastically (e.g. shadows or reflections).&lt;/p&gt;</content><category term="Hobby" /><category term="tech" /><summary>OpenCV is a really powerful tool for computer vision developers. Combined with
python, prototyping and deployment become really fast. I recently revisited some
basic features of OpenCV, and re-discovered this little project I did a few
years ago.</summary></entry><entry><title>Model Predictive Control of a Series Elastic Actuator</title><link href="/post/MPC-SEA/" rel="alternate" type="text/html" title="Model Predictive Control of a Series Elastic Actuator" /><published>2018-01-10T00:00:00+01:00</published><updated>2018-01-10T00:00:00+01:00</updated><id>/post/MPC-SEA</id><content type="html" xml:base="/post/MPC-SEA/">&lt;p&gt;Recently I had the pleasure to work on what is probably the best-made Series
Elastic Actuator (SEA) module in the world: the &lt;a href=&quot;https://www.anybotics.com/anydrive/&quot;&gt;ANYdrive&lt;/a&gt;. from Robotic Systems
Lab at ETH Zürich. I implemented Model Predictive Controller (MPC) for this SEA,
but not a normal MPC. No specs or details about the design of the ANYdrive will
be discussed since those are proprietary. But feel free take a look at &lt;a href=&quot;https://www.anybotics.com/&quot;&gt;ANYbotics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can click on the &lt;a class=&quot;btn btn--info&quot;&gt;blue buttons&lt;/a&gt; to navigate in
this post.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#tag_intro&quot; class=&quot;btn btn--info&quot;&gt;Introduction&lt;/a&gt;
&lt;a href=&quot;#tag_modeling&quot; class=&quot;btn btn--info&quot;&gt;Modeling&lt;/a&gt;
&lt;a href=&quot;#tag_control&quot; class=&quot;btn btn--info&quot;&gt;The Controller&lt;/a&gt;
&lt;a href=&quot;#tag_experiment&quot; class=&quot;btn btn--info&quot;&gt;Experimental Results&lt;/a&gt;
&lt;a href=&quot;#tag_conclusion&quot; class=&quot;btn btn--info&quot;&gt;Conclusion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;tag_intro&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;SEA is a special class of actuators that is considered particularly suitable for
robots that physically interact with uncertain environments. It consists of a
motor and a gearbox, but the load is connected to the output of the gearbox via
a spring. This spring acts as a buffer between the environment and the rather
STIFF gearbox, making the robot compliant (less likely to harm people). The
spring also let you sense torque easily, using Hooke’s Law.&lt;/p&gt;

&lt;p&gt;The ANYdrive is just an example of such actuators. It is drive by a 3-phase
brushless Permanent Magnet Synchronous Motor (PMSM). Instead of using a cascaded
loop to control the PMSM, the PMSM &lt;strong&gt;AND&lt;/strong&gt; the spring in series are controlled
by a single unified controller. Providing an output torque setpoint, the MPC
computes directly the voltage to be applied to the PMSM. This novel MPC is
referred to as the Model Based Unified Torque Controller (MBUTC).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;tag_modeling&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;modeling&quot;&gt;Modeling&lt;/h2&gt;

&lt;p&gt;The first step is to model the ANYdrive. The model contains 2 parts. The first
part is the electrical dynamics of the PMSM, and the second part the the
dynamics of the mechanical system.&lt;/p&gt;

&lt;p&gt;The modeling of the PMSM is well established, and exists in a lot of
literatures. Here, the PMSM is modeled using space vector. All 3-phase
quantities, currents and voltages, and represented in the DQ coordinate system,
which is attached to and rotates with the rotor of the PMSM. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Direct-quadrature-zero_transformation&quot;&gt;Park
Transformation&lt;/a&gt; is used to do this job.&lt;/p&gt;

&lt;p&gt;In DQ coordinate system the dynamics is greatly simplified, and can be writen
down as follows:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/pmsm_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/pmsm_eq.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Equation of the PMSM&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The mechanical system is essentially a spring-mass-damper system, and its
free-body diagram is shown below:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/sea_free_body.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/sea_free_body.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Free-body Diagram of the SEA&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The equation of motion is given by:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/sea_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/sea_eq.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Free-body Diagram of the SEA&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The nonlinear friction effect is rather strong in this case, and has to be
accounted for in the model. Here, the nonlinear friction is modeled as a
hyperbolic tangent function of the motor’s velocity. This function is continuous
and differentiable w.r.t velocity, which is a very desirable property. In the
equation below, Fs is the maximum static friction.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/friction_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/friction_eq.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Modeling of Nonlinear Friction&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now we define the state vector, x, and state space matrices, as follows:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/2018-01-10-MPC-SEA/state_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/state_eq.png&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;/images/2018-01-10-MPC-SEA/action_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/action_eq.png&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;/images/2018-01-10-MPC-SEA/state_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/A_eq.png&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;/images/2018-01-10-MPC-SEA/state_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/B_eq.png&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;/images/2018-01-10-MPC-SEA/state_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/g_eq.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The resulting state space equation of the ANYdrive then looks like this:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/ss_eq.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/ss_eq.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;State Space Model of a PSMS-drive SEA&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In fact all SEAs that are driven by a PMSM can be modeled this way. This model
is therefore potentially applicable to many other SEAs.&lt;/p&gt;

&lt;p&gt;The parameters of the model are identified in a series of experiments. Then, the
response of the real SEA in an experiment is compared to the result of a
numerical simulation using the model (implemented in &lt;a href=&quot;https://ch.mathworks.com/products/simulink.html&quot;&gt;Simulink&lt;/a&gt;). Results
are seen in figures below.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/verify-model-time.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/verify-model-time.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/verify-model-bode.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/verify-model-bode.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Comparison between Experimental Data and Numerical Simulation&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The fact that simulation matches well with experimental data shows the accuracy
of the model.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;tag_control&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-controller&quot;&gt;The Controller&lt;/h2&gt;

&lt;p&gt;The controller is essentially an MPC that controls the deflection of the spring.
The MPC is a form of receding horizon controller. At time step k, the controller
computes optimal actions for time steps k+1 ~ k+N (N is called the horizon).
However, only the action at k+1 is applied to the system, while actions k+2 ~
k+N are discarded. At time step k+1, the above-mentioned step is repeated. The
idea is shown in figure below:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/rhc.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/rhc.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Receding horizon controller&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The following steps are involved at each timestep:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;measurements of the state variables are taken.&lt;/li&gt;
  &lt;li&gt;desired joint torque is converted to the desired spring deflection (using Hooke’s law).&lt;/li&gt;
  &lt;li&gt;the system model derived before is linearized about the measured states.&lt;/li&gt;
  &lt;li&gt;formulate a finite-horizon linear-quadratic-regulator (LQR) for the linearized system.&lt;/li&gt;
  &lt;li&gt;solve this LOR deterministically, using the dynamic programming algorithm.&lt;/li&gt;
  &lt;li&gt;apply the computed optimal action (i.e. voltage) to the motor.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Similar to most MPCs, this control law is relatively computationally expensive.
It was a challenge to implement it in the MCU that controls the SEA, which is
strictly real-time. The algorithm is optimally divided into a number of stages
which run during different hardware interrupt cycles, so the algorithm is able
to run deterministically at the desirable frequency.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;tag_experiment&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;p&gt;The MBUTC is tested in two different setups. First, the output shaft of the SEA
is blocked, so the motor sees an infinite output impedance. Second, a pendulum
of an inertia of approximately 0.1 kg m^2 is attached to the output shaft,
simulating a more realistic loading condition. The setups are shown in figure below:&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/setup-block.jpg&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/setup-block.jpg&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/setup-pendulum.jpg&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/setup-pendulum.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Left: The shaft is blocked. Right: pendulum attached to the shaft.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The step response of the MBUTC is measured. The response with the shaft blocked
is shown as follow:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/step-block.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/step-block.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Torque step response, shaft blocked.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The response with a pendulum attached is shown as follows:&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2018-01-10-MPC-SEA/step-pendulum.png&quot;&gt;&lt;img src=&quot;/images/2018-01-10-MPC-SEA/step-pendulum.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;tag_conclusion&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The modeling and system identification of the SEA are satisfactory. However, the
MBUTC performs worse than the traditional PID controller with cascaded loops.&lt;/p&gt;

&lt;p&gt;A number of possible explanations of the sub-optimal performance:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the MBUTC does not have a integrator action to eliminate steady-state error.&lt;/li&gt;
  &lt;li&gt;the MBUTC is too short-sighted, due to the finite (and short) horizon used here.&lt;/li&gt;
  &lt;li&gt;the model of static friction is not perfect (the model is continuous w.r.t velocity, but the real static friction is not).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The most promising candidate for improvement is to add an disturbance observer
to this MPC. An disturbance observer will help eliminating steady-state error,
and may be able to mitigate the effect of imperfect friction model.&lt;/p&gt;</content><category term="tech" /><category term="life" /><summary>Recently I had the pleasure to work on what is probably the best-made Series
Elastic Actuator (SEA) module in the world: the ANYdrive. from Robotic Systems
Lab at ETH Zürich. I implemented Model Predictive Controller (MPC) for this SEA,
but not a normal MPC. No specs or details about the design of the ANYdrive will
be discussed since those are proprietary. But feel free take a look at ANYbotics.</summary></entry><entry><title>Dairy</title><link href="/post/Dairy-2017-2018/" rel="alternate" type="text/html" title="Dairy" /><published>2017-02-25T00:00:00+01:00</published><updated>2017-02-25T00:00:00+01:00</updated><id>/post/Dairy-2017-2018</id><content type="html" xml:base="/post/Dairy-2017-2018/">&lt;h2 id=&quot;april-2018&quot;&gt;April 2018&lt;/h2&gt;

&lt;p&gt;Got a fancy new toy for my master thesis :)&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/Life-Collection/2018-03-mabi.jpg&quot;&gt;&lt;img src=&quot;/images/Life-Collection/2018-03-mabi.jpg&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;sep-2017&quot;&gt;Sep 2017&lt;/h2&gt;
&lt;p&gt;An interesting jorney has ended, and another one must start. Having finished the internship at Maxon Motor, it’s time to go back to school and be academic again :)&lt;/p&gt;

&lt;p&gt;About the internship, there is so much to love about it. I’ve got a lovely workplace, friendly and helpful colleagues, and a ton of interesting motors to work with. Now I’ve really got to know motors and drivers as well as my fingers, and my German did improve a lot. Not to mention that at the end of the job my colleagues gave me the most special gift I’ve ever received! Can’t love it more.&lt;/p&gt;

&lt;p&gt;Worth mentioning, I’ve also started a project of my own, to develop a motor control platform. The target is to develop a controller for 3-phase PMSM, capable of executing current, speed, and position control. It will be able to control a series elastic actuator (SEA). Development has already started on a development kit, and the design of in-house hardware platform is underway. Hopefully a full solution can be turned out next year.&lt;/p&gt;

&lt;p&gt;Back in school, still a ton of work is waiting for me. A few courses, my small thesis, and finding a job. The small thesis is particularly interesting for me. It’s about the modeling and control of a series elastic actuator (SEA), which is a special type of motor that’s particularly useful for robots that have to interact safely with human. Given the boom in cobot market that everybody envisions, SEA is a technology with big potentials. Really excited to work on it!&lt;/p&gt;

&lt;h2 id=&quot;feb-2017&quot;&gt;Feb 2017&lt;/h2&gt;
&lt;p&gt;End of February is the begining of a new journey! Last December I got an offer for an internship at the world-famous Maxon Motor AG! Excited for quite a while, and I’m soon going to start it! Really like the company’s nice location and huge lab, dream of all engineers~.&lt;/p&gt;

&lt;p&gt;My part-time job at RSL has also come to an end. Must say it’s a nice experience, in which I learned a ton of things about electronics, system integration, team working, and much more. The people are supper nice and supportive, a great part of the job. Although it’s a bit stressful sometimes, it’s not too bad, and I’m happy in the end. Hope to do my semester project or thesis with them, would be a great option!&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/Life-Collection/2017-01-maxon.jpg&quot;&gt;&lt;img src=&quot;/images/Life-Collection/2017-01-maxon.jpg&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;jan-2017&quot;&gt;Jan 2017&lt;/h2&gt;
&lt;p&gt;Have been doing a part-time job at ETH’s famous Robotic System Lab (RSL) for a few months, a fantastic experience! RSL is among the world’s best labs that study quadrupedal robots, or “robotic dogs” as I like call them. It’s really exciting to see it walk and jump. Digging deep into the low-level control systems for the robot is also full of fun.&lt;/p&gt;
&lt;figure&gt;
    &lt;a href=&quot;/images/Life-Collection/2017-01-me-with-dog.jpg&quot;&gt;&lt;img src=&quot;/images/Life-Collection/2017-01-me-with-dog.jpg&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;</content><category term="Hobby" /><category term="life" /><summary>April 2018</summary></entry><entry><title>My First Home Made HiFi Audio Decoder and Amplifier</title><link href="/post/Audio-Decoder/" rel="alternate" type="text/html" title="My First Home Made HiFi Audio Decoder and Amplifier" /><published>2016-07-28T00:00:00+02:00</published><updated>2016-07-28T00:00:00+02:00</updated><id>/post/Audio-Decoder</id><content type="html" xml:base="/post/Audio-Decoder/">&lt;p&gt;Recently I developed my first in-house hi-fi audio decoder + amplifier project. Having tested it with my own ears I was very much surprised by the outstanding sound quality. Considering the fact that this project has costed me less than 300 HKD (thanks to cheap PCBs from Shenzhen), I am sooooo happy with the result! It has a name: “JAudio Decoder Amp”&lt;/p&gt;

&lt;h2 id=&quot;the-design&quot;&gt;The Design&lt;/h2&gt;

&lt;p&gt;To realise this project with simple techniques and low budget, I selected a number of good-quality yet easy-to-use chips to base my design on.&lt;/p&gt;

&lt;p&gt;I used &lt;a href=&quot;http://www.ti.com/product/PCM2707&quot;&gt;PCM2707&lt;/a&gt; computer interface chip + decoder + DAC. PCM2707 is a USB audio interface that enables music playback through USB. In addition to that it also has a built-in DAC that can convert 16-bit 44 kHz sound source with down to 0.006% THD into line. 16-bit 44kHz is standard CD-quality sound source, which I believe is the highest quality sound source that I can get most of the time. So I would not bother myself to use higher-quality DAC chips, which costs more and makes the design more complicated.&lt;/p&gt;

&lt;p&gt;The built-in DAC of PCM 2707 can directly drive headphones with 32 ohm impedance, but the THD would be higher this way. Wanting to further improve sound quality, I used &lt;a href=&quot;http://www.ti.com/product/TPA6120A2&quot;&gt;TPA6120&lt;/a&gt; audio amplifier as output buffer. The amplifier is supplied with +5V ~ -5V, and the gain is set to 1.5V/V.&lt;/p&gt;

&lt;p&gt;To generate the -5V required to drive TPA6120, a switch mode voltage converter, LM2594, was employeed to convert +5V (from USB Vbus) to -5V.&lt;/p&gt;

&lt;p&gt;All 3 devices come from IT. Check data sheets for details and application information.&lt;/p&gt;

&lt;p&gt;The complete and most updated schematic is shown in Fig.1, alone with some layout considerations.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/sch.png&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/sch.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.1 Schematic&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;the-pcb&quot;&gt;The PCB&lt;/h2&gt;
&lt;p&gt;The PCB layout was created based on the schematic in Fig.1. It’s a 2-layer PCB, and it features separated ground panels for digital and analog circuitries, which helps reduce cross-over noise. According to the data sheet of TPA60120, for stable operation and optimum performance, there should be no ground panel underneath the output and feed-back pins of the IC. This recommendation was very well implemented. In addition, the output and feed-back paths were made as short as possible.&lt;/p&gt;

&lt;p&gt;The design of the PCB is shown in Fig.2 and Fig.3.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/pcb-top.png&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/pcb-top.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/pcb-bot.png&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/pcb-bot.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.2 PCB, top layer&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.3 PCB, bottom layer&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;the-final-product&quot;&gt;The Final Product&lt;/h2&gt;
&lt;p&gt;After a few hours of assembly and some hardware debugging, my computer finally successfully recognized the chip, and sound flowed out like water. It has a crystal clear high-frequency, and rich mid-frequency, and decent low-frequency. The response is quite flat, being totally loyal to the original recording. When the music is off, there is no noise what so ever. I made a 5-minute comparison between my laptop’s audio output and this decoder. There is subtle differences, and the later offers clearer sound. From first impression I like my decoder better.&lt;/p&gt;

&lt;p&gt;I used my Beyerdynamics DT880 headphone (32ohm version) for the above testings.&lt;/p&gt;

&lt;p&gt;Haven’t washed the PCB yet, still covered with flux. But here is the final product (Fig.4 and 5).&lt;/p&gt;
&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/final-product.jpg&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/final-product.jpg&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/testing.jpg&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/testing.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.4 (Left) JAudio Decoder Amp, Finished Product&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.5 (Right) Listening Test&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are minor problems with the board. First, sometimes my computer fails to recognize PCM2707, because of power supply instability at power-up. PCM2707 is supplied with 5V USB bus voltage, and its internal voltage regulator generates the 3.3V voltage needed for its internal operations. The internal voltage regulator, however, occasionally exhibits instability at power-up, causing the device to malfunction. The cause of the instability is the inadequate value of decoupling capacitor. I added as much as I could, and the device could function normally 90% of the time. Second, the power and status indicator LEDs have different brightnesses, which is kind of annoying.&lt;/p&gt;

&lt;p&gt;I made a cute little case for my decoder and amplifier, using a Snoopy pencil case. Look very nice, feel better to the touch!&lt;/p&gt;
&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/case-1.jpg&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/case-1.jpg&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-07-31-Audio-Decoder/case-2.jpg&quot;&gt;&lt;img src=&quot;/images/2016-07-31-Audio-Decoder/case-2.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.6 (Left) JAudio Decoder Amp, In a Lovely Case&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.7 (Right) In Use&lt;/figcaption&gt;
&lt;/figure&gt;</content><category term="Hobby" /><category term="DIY" /><summary>Recently I developed my first in-house hi-fi audio decoder + amplifier project. Having tested it with my own ears I was very much surprised by the outstanding sound quality. Considering the fact that this project has costed me less than 300 HKD (thanks to cheap PCBs from Shenzhen), I am sooooo happy with the result! It has a name: “JAudio Decoder Amp”</summary></entry><entry><title>UAV Communication Relay for another UAV</title><link href="/post/UAV-Communication-Relay-for-another-UAV/" rel="alternate" type="text/html" title="UAV Communication Relay for another UAV" /><published>2016-05-30T00:00:00+02:00</published><updated>2016-05-30T00:00:00+02:00</updated><id>/post/UAV-Communication-Relay-for-another-UAV</id><content type="html" xml:base="/post/UAV-Communication-Relay-for-another-UAV/">&lt;p&gt;This project is my undergraduate final year project, and I wrote my undergraduate thesis out of it. Hopefully this short article will give you a brief idea about what I’ve done. This project was a teamwork, I could not complete it without my team mates Cai Lingfeng and Wan Hiu Fung. I would also like to thank our supervisor Prof Wen Chih Yung, an absolutely nice and helpful prof.&lt;/p&gt;

&lt;p&gt;This project has helped us win &lt;strong&gt;champion&lt;/strong&gt; at the &lt;a href=&quot;http://www.iaa.ncku.edu.tw/~whlai/uav/2016/index.html&quot;&gt;2016 Taiwan Unmanned Aircraft Design Competition&lt;/a&gt; (website in Chinese), in which we take great pride. we are also looking to publish a paper based on it.&lt;/p&gt;

&lt;h2 id=&quot;the-idea&quot;&gt;The Idea&lt;/h2&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Why do we need radio communication ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your UAV goes out for a mission, you want to make sure that you can contact the UAV at any time, to send commands to it, and to know what it is doing. There are two reasons for this. The first one is also the most obvious: if you cannot even contact your UAV, how can you do the mission with it? The second one is, if you lose contact with your UAV, your UAV may do unexpected actions without you knowing it, such actions can damage properties, and can ever hurt or kill somebody.&lt;/p&gt;

&lt;p&gt;The choice of radio over other technologies such as WiFi or 3G/4G mobile network is made for 2 reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Radio has longer range than WiFi &lt;/li&gt;
  &lt;li&gt;Radio has shorter (and more predictable) delay than 3G/4G &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although radio generally has lower data rates.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Why relaying radio communication ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your UAV does a mission in areas where you cannot directly, does it mean you have to abort the mission? With traditional UAV systems, yes. But with radio communication relay, NO! With the help of radio communication relay, the UAV can fly into areas where direct radio communication with you is blocked.&lt;/p&gt;

&lt;p&gt;This idea is illustrated in Fig.1. If radio communication is blocked by, say, a mountain, then the use of relay can easily help us get arround the obstacle. In addition, relay can also help extend the range of communication, so you can go further with a radio transceiver of smaller power.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/idea.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/idea.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.1 UAV Radio Communication Relay&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;system-architecture&quot;&gt;System Architecture&lt;/h2&gt;

&lt;p&gt;To demonstrate radio communication relay, a small UAV was constructed using 2 UAVs and a Ground Control Station, which we call “Mini GCS”. As shown in Fig.2, names are given to the UAVs: Mom is the one that relays communication at the middle, and Son is the one that actually executes the mission. Fig.3 shows the critical subsystems of the demonstration UAV system.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mom_son.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mom_son.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/sys_block.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/sys_block.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.2 (Left) Mom, Son, and Mini GCS&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.3 (Right) Subsystems&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt; Communication &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;915MHz Radio telemetry by 3DR (3DR Radio) was used to construct the communication network. A total of 4 3DR Radios were used to establish 2 independent radio data links, one between &lt;span style=&quot;color:#008000&quot;&gt; Mom and GCS&lt;/span&gt;, and the other between &lt;span style=&quot;color:#008000&quot;&gt; Mom and Son&lt;/span&gt; (Fig.4). The 2 radio links are kept interference-free using the Frequency Hopping (FHSS) technique, which is a built-in feature of 3DR Radio. The 2 radio links use different frequency hopping sequence, which are determined by a unique “Net ID” (shown in Fig.5).&lt;/p&gt;

&lt;figure class=&quot;third&quot;&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/radio.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/radio.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/radio_setting.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/radio_setting.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/frame.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/frame.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.4 (Left) Radio Communication Architecture&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.5 (Mid) Frequency Hopping Settings&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.6 (Right) Format of Data Frame&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In order to detect errors in data transmission, a Cyclic Redundancy Checking (CRC) error checking stage was added. To do this, data is organized into frames, and the frames have fixed format (Fig.6). At then end of each frame, a 4-byte CRC code is attached. When a frame is received by UAV or GCS, the receiver will first record the CRC code received over radio (CRC_remote), and then compute a new CRC code based on the received data (CRC_local). If CRC_remote is equal to CRC_local, then the frame is error-free. Otherwise, the received frame has error(s) in it and has to be discarded. More details about CRC can be found &lt;a href=&quot;https://en.wikipedia.org/wiki/Cyclic_redundancy_check&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt; Mission Control &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This mission control unit is used to do a number of jobs:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It handles message routing and communication relay.&lt;/li&gt;
  &lt;li&gt;It interfaces GCS to flight controller. User can send relatively simple commands from GCS, then mission control translates them into the low-level instructions to the flight controller.&lt;/li&gt;
  &lt;li&gt;It is supposed to control the mission with intelligence, sadly we still haven’t implemented that (will we ?).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The mission control unit is in fact a software system, developed with &lt;a href=&quot;http://www.ros.org/&quot;&gt;ROS&lt;/a&gt;, than runs on a &lt;a href=&quot;https://www.raspberrypi.org/products/raspberry-pi-2-model-b/&quot;&gt;Raspberry Pi 2B&lt;/a&gt;. The mission control unit communicates with radio telemetries and flight controller through UART ports. So it means as long as you are using a UAV with supported flight controller (in our case, the &lt;a href=&quot;https://pixhawk.org/choice&quot;&gt;PixHawk Flight Controller&lt;/a&gt;), then you can plug this mission control unit into it and turn it into an on-air communication relay station! The system is totally platform independent, which is cool and convenient.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt; Flight Control &amp;amp; Aircraft &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is nothing very special with the actual aircraft used. For testing, we built 2 quad-rotor copters out of off-the-shelf components, and used them as Mom and Son. For flight controller, we selected &lt;a href=&quot;https://pixhawk.org/choice&quot;&gt;PixHawk Flight Controller&lt;/a&gt; because it is powerful and plenty of resources are available online. Fig.8 shows the UAVs during a field flight test.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mission-control.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mission-control.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/test.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/test.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.7 (Left) Mission Control Unit Mounted on UAV&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.8 (Right) UAV Mom and Son, during a test&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;strong&gt; Ground Control Station (GCS) &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We developed our own version of GCS software, which we call “Mini GCS”. It was writen fully in-house using C#. A screen shoot is given in Fig.9 The Mini GCS has a number of cool features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A UAV status window, which allows you to monitor BOTH UAVs at the same time.&lt;/li&gt;
  &lt;li&gt;A map display, that shows the location of both UAVs in real time.&lt;/li&gt;
  &lt;li&gt;A Linux command line style interface, in which you can type commands and send them to the UAV that you specify, Mom or Son.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A number of useful commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arm              be ready to fly, engage motors
takeoff	         takeoff and fly to specified height
land             land immediately
setmode          change to flight mode of the UAV
rtl              return to home position, and land there
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mini-gcs.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/mini-gcs.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.9 Mini GCS screen shoot (UAVs not yet displayed on map)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;testing-and-results&quot;&gt;Testing and results&lt;/h2&gt;

&lt;p&gt;The video below shows how we tested the communication relay function. First, we let “Son” fly far enough so radio communication with GCS breaks (because radio telemetries are out of range). Then, we deploy “Mom” to serve as relay, after which radio communication of “Son” with GCS resumes. Communication status is indicated by the “Connected/Disconnected” icon	 in the bottom-right corner of the video, which turns &lt;span style=&quot;color:#00F000&quot;&gt;Green&lt;/span&gt; when communication is healthy, and turns &lt;span style=&quot;color:#F00000&quot;&gt;Red&lt;/span&gt; when communication breaks.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/HxN0oafNmzw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;+&lt;/p&gt;

&lt;p&gt;Fig.10 shows the variation of the radio’s signal and noise levels between “Son” and GCS during a typical mission (measurements not taken during the test shown in the video, but during a later test). As we can see, from t = 0 to t = t1, as “Son” started its mission and its distance to GCS increased, radio signal level dropped steadily. At t = t1, the radio signal level was almost equal to the noise level, implying a lost to radio communication. At this moment, “Mom” was started and deployed to the designated relay position (between “Son” and GCS). as “Mom” approached “Son” and the relay started to function, the signal level gradually rose again. At t = t2, signal level was above noise level, implying that radio communication between “Son” and GCS was re-established.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/RSSI.png&quot;&gt;&lt;img src=&quot;/images/2016-05-30-UAV-Communication-Relay-for-another-UAV/RSSI.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.10 Radio signal &amp;amp; noise level, between &quot;Son&quot; and GCS&lt;/figcaption&gt;
&lt;/figure&gt;</content><category term="tech" /><summary>This project is my undergraduate final year project, and I wrote my undergraduate thesis out of it. Hopefully this short article will give you a brief idea about what I’ve done. This project was a teamwork, I could not complete it without my team mates Cai Lingfeng and Wan Hiu Fung. I would also like to thank our supervisor Prof Wen Chih Yung, an absolutely nice and helpful prof.</summary></entry><entry><title>Driver for MPU6050 IMU Sensor, and Pose Estimation with Kalman Filter</title><link href="/post/MPU6050/" rel="alternate" type="text/html" title="Driver for MPU6050 IMU Sensor, and Pose Estimation with Kalman Filter" /><published>2016-05-10T00:00:00+02:00</published><updated>2016-05-10T00:00:00+02:00</updated><id>/post/MPU6050</id><content type="html" xml:base="/post/MPU6050/">&lt;p&gt;&lt;a href=&quot;https://www.invensense.com/products/motion-tracking/6-axis/mpu-6050/&quot;&gt;MPU6050&lt;/a&gt;
is a low-cost IMU sensor, which provides 3-axis acceleration and 3-axis angular
velocity measurements (6 DOF in total). This sensor is really handy when it
comes to implementing things like wearable devices and drones. I got an
evaluation module of this sensor, and decided to build a very simple interface
for it using a 8-bit MCU. Source code is available at &lt;a href=&quot;https://github.com/YifanJiangPolyU/MPU6050&quot;&gt;https://github.com/YifanJiangPolyU/MPU6050&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A video showing how it works:&lt;/p&gt;

&lt;iframe width=&quot;1294&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/-LPgOFSReEU&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;MPU6050 has build-in ADC and is capable of outputting readings digitally via a
I2C interface. In this interface, a Arduino Nano, which has a 8MHz 8-bit MCU, is
used as the I2C master. The Arduino reads measurements from MPU6050 cyclically,
and estimates the pose of the sensor, represented as Euler angles, using
a Kalman filter.&lt;/p&gt;

&lt;p&gt;The estimated Euler angles are then transmitted to the PC via serial port (i.e.
UART). On the PC side, a python script reads the pose data and visualizes it in
real time.&lt;/p&gt;

&lt;p&gt;The current implementation has a number of problems, but improvements are also easy to implement.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The 8-bit MCU is not powerful enough to execute pose estimation.&lt;/li&gt;
  &lt;li&gt;Delay is sever, because of inadequate processing power of the MCU.&lt;/li&gt;
  &lt;li&gt;Communication protocol between Arduino and PC is too primitive. Better protocol can be used to enable data integrity checking and miss data detection.&lt;/li&gt;
  &lt;li&gt;The data acquisition and the Kalman filter run in an infinite loop in &lt;code class=&quot;highlighter-rouge&quot;&gt;main()&lt;/code&gt;. A better way would be to make this interrupt-triggered, for a more deterministic sample time.&lt;/li&gt;
&lt;/ol&gt;</content><category term="Hobby" /><category term="tech" /><summary>MPU6050
is a low-cost IMU sensor, which provides 3-axis acceleration and 3-axis angular
velocity measurements (6 DOF in total). This sensor is really handy when it
comes to implementing things like wearable devices and drones. I got an
evaluation module of this sensor, and decided to build a very simple interface
for it using a 8-bit MCU. Source code is available at https://github.com/YifanJiangPolyU/MPU6050</summary></entry><entry><title>Swing-up and Balancing Control of an Inverted Pendulum</title><link href="/post/Inverted-Pendulum/" rel="alternate" type="text/html" title="Swing-up and Balancing Control of an Inverted Pendulum" /><published>2016-04-01T00:00:00+02:00</published><updated>2016-04-01T00:00:00+02:00</updated><id>/post/Inverted-Pendulum</id><content type="html" xml:base="/post/Inverted-Pendulum/">&lt;p&gt;In this mini project, nonlinear control strategies for the swing-up control of
an inverted pendulum on a cart (i.e. the cart-pole system) is explored. The
controller is verified in a simulation. Let’s take a look at how it works in the
video below. Only the pendulum is visualized, and the cart is not visible.&lt;/p&gt;

&lt;iframe width=&quot;1294&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/SJh4P6uV5ag&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Source code for this project is available at &lt;a href=&quot;https://github.com/YifanJiangPolyU/cart-pole-nonlinear-control&quot;&gt;https://github.com/YifanJiangPolyU/cart-pole-nonlinear-control&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-system&quot;&gt;The System&lt;/h2&gt;

&lt;p&gt;A cart-pole system is a classical example used in control engineering. It is something that looks like Figure 1.&lt;/p&gt;

&lt;figure style=&quot;width: 360px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2016-04-01-Inverted-Pendulum/cart-pole.png&quot; /&gt;
  &lt;figcaption&gt;Fig 1. The Cart Pole System (&lt;a href=&quot;https://danielpiedrahita.wordpress.com/portfolio/cart-pole-control/&quot;&gt;source&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A pendulum (the pole) freely rotates on a shaft mounted on a moving cart. The
shaft is NOT actuated, and the only control action applicable is a force on the
cart.&lt;/p&gt;

&lt;h2 id=&quot;the-algorithm&quot;&gt;The Algorithm&lt;/h2&gt;

&lt;p&gt;Initially, the pendulum is at its stable equilibrium position (i.e. lowest
position). The control algorithm does 2 jobs. First, it tires to move the cart
so that the pendulum swings up to the unstable equilibrium position (i.e. the
highest position). Second, it tries to balance the pendulum at its unstable
equilibrium.&lt;/p&gt;

&lt;p&gt;The swing-up part of the job is more interesting and more challenging. The
during swing-up, the pendulum travels through highly nonlinear regions of the
state space, rendering conventional linear system techniques unusable. Further
more, the system is under-actuated, and when the pendulum is horizontal, moving
the cart does not generate any effect on the pendulum (even for a very brief
moment).&lt;/p&gt;

&lt;p&gt;The method implemented here was mentioned in the course materials of &lt;a href=&quot;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-832-underactuated-robotics-spring-2009/&quot;&gt;MIT
OpenCourseWare 6.832, Underactuated Robotics&lt;/a&gt; This method transforms the
swing-up control problem into the regulation problem of an energy-like term.
Namely, the energy error term is defined as follows:&lt;/p&gt;

&lt;figure style=&quot;width: 254px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/images/2016-04-01-Inverted-Pendulum/energy-error.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;In the equation, J is the moment of inertia of the pendulum, and g is the
gravitational acceleration. Other notations are consistent with Figure 1. Note
that when the pendulum is at upright position, the energy error term goes to
zero. A regulator is then added to drive the energy error to zero, which is
effectively the same as making the pendulum approach the upright position.&lt;/p&gt;

&lt;p&gt;When the pendulum is close enough to the upright position, the system can
effectively be replaced by its linearization about the upright position. At this
moment, the energy error regulator is simply switched off and a linear quadratic
regulator (LQR) takes over to balance the pendulum.&lt;/p&gt;</content><category term="Hobby" /><category term="tech" /><summary>In this mini project, nonlinear control strategies for the swing-up control of
an inverted pendulum on a cart (i.e. the cart-pole system) is explored. The
controller is verified in a simulation. Let’s take a look at how it works in the
video below. Only the pendulum is visualized, and the cart is not visible.</summary></entry><entry><title>Building a 3-Phase Servo Motor Driver</title><link href="/post/Servo-Motor-Driver-Project/" rel="alternate" type="text/html" title="Building a 3-Phase Servo Motor Driver" /><published>2014-05-10T00:00:00+02:00</published><updated>2014-05-10T00:00:00+02:00</updated><id>/post/Servo-Motor-Driver-Project</id><content type="html" xml:base="/post/Servo-Motor-Driver-Project/">&lt;p&gt;In the past few months I was intensively exposed to servo motors: their constructions, working principles, and driving circuits, thanks to my internship. These motors are brushless, driven by 3-phase AC currents, and when properly controlled (as in my internship company), can execute motion with nanometer-level of resolution. The science and technology behind these motors deeply fascinated me, which inspired me to try building my own 3-phase servo motor driver, one of the core components in the motor’s control.&lt;/p&gt;

&lt;p&gt;The 3-phase servo motor driver is essentially a 3-phase DC-to-AC converter, which provides current measurement and feedback for control purpose. The major functional blocks of the driver are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3-Phase MOSFET inverter and MOSFET gate drivers.&lt;/li&gt;
  &lt;li&gt;Phase Current Sensing.&lt;/li&gt;
  &lt;li&gt;Power Supply System.&lt;/li&gt;
  &lt;li&gt;Feedback Signal Conditioning &amp;amp; ADC (Optional).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, this driver will also provide complete isolation between high-power circuit and MCU control circuit (MCU control circuit is external to the driver).&lt;/p&gt;

&lt;h3 id=&quot;the-3-phase-inverter&quot;&gt;The 3-Phase Inverter&lt;/h3&gt;

&lt;p&gt;The 3-phase inverter circuit consists of 3 half-bridge circuits, which controls the 3 phases of the motor. Each half-bridge is made up of 2 MOSFETs (I used Fairchild 40N25). the inverter circuit is shown in Fig.1. The inverter is powered from V+, which is the motor’s power supply with user-defined voltage (according to needs, max 100V DC).&lt;/p&gt;

&lt;p&gt;Because both high- and low-side of the half-bridge uses N-channel MOSFET, and becuase of the characteristics of N-Channel MOSFET, a gate driving circuit becomes necessary. I used IR2183 integrated half-bridge driver to do this job. The driver circuit (for one phase only) is shown in Fig.2. Opto-coupler HCPL0630 was used to isolate the high-power driver circuit from MCU. Sorry but please notice that value of R7 and R8 shown in Fig.2 (1K ohm) is &lt;span style=&quot;color:#F00000&quot;&gt;WRONG&lt;/span&gt; because I misunderstood some datasheets. The value for R7 and R8 in the final design is 330 ohm.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/inverter.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/inverter.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/gate-driver.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/gate-driver.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.1 (Left) 3-Phase Inverter&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.2 (Right) MOSFET Gate Driver (1 phase only, same for all 3 phases)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To make sure the design works, I built a prototype of the inverter circuit (1 phase only) and tested it (shown in Fig.3 and Fig.4). Fig.4 shows the halg-bridge switching an inductive load with approx. 70% duty. The MOSFET gate driver circuit (Fig.2) does not operate when PWM duty is below approx. 3% or above approx 95%.&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-gate-driver-1.jpg&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-gate-driver-1.jpg&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-gate-driver-2.jpg&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-gate-driver-2.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.3 (Left) Test Setup&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.4 (Right) MOSFET Switching Waveform&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;phase-current-sensing&quot;&gt;Phase Current Sensing&lt;/h3&gt;

&lt;p&gt;The job of the current sensing circuit is to accurately measure the amount of current flowing in each phase. The measurement is then provided MCU, which executes a close-loop control algorithm that controls the current flow in phases (the current loop). I used LTSR-25NP current transducer to sense the phase currents. LTSR-25NP is a hall-effect current censor, and its output is completely isolated from the current that it measures. It helps isolate high- and low-power parts of the driver board.&lt;/p&gt;

&lt;h3 id=&quot;power-supply-system&quot;&gt;Power Supply System&lt;/h3&gt;

&lt;p&gt;In my driver, the motor is powered from V+ (user-defined, max 100V DC). However, the rest of the system: MOSFET gate drivers, current sensors, etc, are powered from a separate power source, fixed at 24V DC. This 24V voltage is converted by switch-mode voltage regulators into 2 different voltages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;15V: power supply to MOSTEF gate drivers&lt;/li&gt;
  &lt;li&gt;5V: for high-power circuits that needs 5V supply (e.g. opto-isolator, high-power side).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, there is another 5V linear regulator, which supplies power for low-power circuits that needs 5V: opto-isolator low-power side, current transducers, etc.&lt;/p&gt;

&lt;p&gt;An additional 5V voltage, VREF, is provided by REF195 (by Analog Devices). this is the precision voltage reference for an the onboard ADC (which was never tested, sadly)&lt;/p&gt;

&lt;p&gt;The power supply curcuit is shown in Fig.6.&lt;/p&gt;

&lt;figure&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/power-supply.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/power-supply.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.6 Power Supply&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;feedback--signal-conditioning--adc&quot;&gt;Feedback  Signal Conditioning &amp;amp; ADC&lt;/h3&gt;

&lt;p&gt;to be added&lt;/p&gt;

&lt;h3 id=&quot;pcb-design&quot;&gt;PCB Design&lt;/h3&gt;

&lt;p&gt;My driver board features 4-layer PCB, with middle layers dedicated to GND and power supply. In the layout I paid attention to keep the ground panel for high- and low-power circuits separated. Total isolation is achieved by using opto-isolators and isolated current sensors. The Layers are shown in Fig.7 ~ Fig.10, and the completed PCB is shown in Fig.11(front side) and Fig.12(back side).&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/L1.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/L1.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/supply.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/supply.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.7 (Left) Front Side Gerber&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.8 (Right) Supply Layer Gerber&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/gnd.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/gnd.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/L4.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/L4.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.9 (Left) Ground Panel Gerber&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.10 (Right) Back Side Gerber&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;half&quot;&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/front-detail.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/front-detail.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/back-detail.png&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/back-detail.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.11 (Left) Finished PCB, Front Side&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.12 (Right) Finished PCB, Back Side&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Fig.13 shows the finished motor driver in use, driving a brushed DC motor (only one of the 3 phases is used).&lt;/p&gt;
&lt;figure&gt;
    &lt;a href=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-product.jpg&quot;&gt;&lt;img src=&quot;/images/2014-05-10-Servo-Motor-Driver-Project/test-product.jpg&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.13 Driving a Brushed DC Motor&lt;/figcaption&gt;
&lt;/figure&gt;</content><category term="tech" /><category term="DIY" /><category term="hobby" /><summary>In the past few months I was intensively exposed to servo motors: their constructions, working principles, and driving circuits, thanks to my internship. These motors are brushless, driven by 3-phase AC currents, and when properly controlled (as in my internship company), can execute motion with nanometer-level of resolution. The science and technology behind these motors deeply fascinated me, which inspired me to try building my own 3-phase servo motor driver, one of the core components in the motor’s control.</summary></entry><entry><title>Learning Logic Circuits with Minecraft</title><link href="/post/Minecraft/" rel="alternate" type="text/html" title="Learning Logic Circuits with Minecraft" /><published>2013-06-10T00:00:00+02:00</published><updated>2013-06-10T00:00:00+02:00</updated><id>/post/Minecraft</id><content type="html" xml:base="/post/Minecraft/">&lt;p&gt;I’m not a game person, and I rarely play games. However, there’s one game to which I’m particularly addicted: Minecraft!!!&lt;/p&gt;

&lt;p&gt;Minecraft is a game which allows you to build things in it, using pre-defined blocks. There are so many types of blocks and so many ways to combine them that there is virtually no limit on what you can build: houses, gardens, weapons, trains, and even computers! Minecraft has so many interesting aspects, find out by yourself at &lt;a href=&quot;https://minecraft.net/&quot;&gt;their homepage&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;At some point, I noticed how interesting it was to construct logic circuits in minecraft. After some experiments, I built my first logic gate! (Fig.1) However, my own design was too bulky, so I just adopted logic gates provided in Minecraft’s official Wiki.&lt;/p&gt;

&lt;p&gt;Fig.2 shows a 8-bit binary adder, and Fig.3 shows a 4-bit binary multiplier, both “designed in-house”. I had a plan to build a simple functioning calculator, but never had time to complete it. Don’t be surprised about the animals that are walking on the circuits, they are auto-generated by the game and do not do any harm.&lt;/p&gt;

&lt;figure class=&quot;third&quot;&gt;
    &lt;a href=&quot;/images/2013-06-10-Minecraft/gate.png&quot;&gt;&lt;img src=&quot;/images/2013-06-10-Minecraft/gate.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2013-06-10-Minecraft/adder.png&quot;&gt;&lt;img src=&quot;/images/2013-06-10-Minecraft/adder.png&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;/images/2013-06-10-Minecraft/multiplier.png&quot;&gt;&lt;img src=&quot;/images/2013-06-10-Minecraft/multiplier.png&quot; /&gt;&lt;/a&gt;
    &lt;figcaption&gt;Fig.1 (Left) OR gate, my design&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.2 (Mid) 8-bit binary adder/subtractor&lt;/figcaption&gt;
    &lt;figcaption&gt;Fig.3 (Right) 4-bit binary multiplier&lt;/figcaption&gt;
&lt;/figure&gt;</content><category term="life" /><category term="hobby" /><summary>I’m not a game person, and I rarely play games. However, there’s one game to which I’m particularly addicted: Minecraft!!!</summary></entry></feed>
